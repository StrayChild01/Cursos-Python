{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In this Notebook we will examine bitcoin prices and see if we can predict the value.\n",
    "\n",
    "https://www.kaggle.com/mczielinski/bitcoin-historical-data/data\n",
    "\n",
    "Data under CC BY-SA 4.0 License\n",
    "\n",
    "https://www.kaggle.com/mczielinski/bitcoin-historical-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble, linear_model, model_selection, preprocessing, svm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Resampling data from minute interval to day\n",
    "bit_df = pd.read_csv('../data/coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv')\n",
    "# Convert unix time to datetime\n",
    "bit_df['date'] = pd.to_datetime(bit_df.Timestamp, unit='s')\n",
    "# Reset index\n",
    "bit_df = bit_df.set_index('date')\n",
    "# Rename columns so easier to code\n",
    "bit_df = bit_df.rename(columns={'Open':'open', 'High': 'hi', 'Low': 'lo', \n",
    "                       'Close': 'close', 'Volume_(BTC)': 'vol_btc',\n",
    "                       'Volume_(Currency)': 'vol_cur', \n",
    "                       'Weighted_Price': 'wp', 'Timestamp': 'ts'})\n",
    "# Resample and only use recent samples that aren't missing\n",
    "bit_df = bit_df.resample('d').agg({'open': 'first', 'hi': 'max', \n",
    "    'lo': 'min', 'close': 'last', 'vol_btc': 'sum',\n",
    "    'vol_cur': 'sum', 'wp': 'mean', 'ts': 'min'}).iloc[-1000:]\n",
    "# drop last row as it is not complete\n",
    "bit_df = bit_df.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bit_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.close.plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Load data\n",
    "This exercise looks at predicting the size of forest fires based on meteorological data https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "\n",
    "The file is in ``../data/forestfires.csv``\n",
    "\n",
    "* Read the data into a DataFrame\n",
    "* Examine the types\n",
    "* Describe the data\n",
    "\n",
    "Attribute information:\n",
    "\n",
    "   For more information, read [Cortez and Morais, 2007].\n",
    "\n",
    "   1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "   2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "   3. month - month of the year: \"jan\" to \"dec\" \n",
    "   4. day - day of the week: \"mon\" to \"sun\"\n",
    "   5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "   6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "   7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "   8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "   9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "   10. RH - relative humidity in %: 15.0 to 100\n",
    "   11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "   12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "   13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "   (this output variable is very skewed towards 0.0, thus it may make\n",
    "    sense to model with the logarithm transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we predict tomorrow's close based on today's info?\n",
    "We will use a row of data for input. We will call the input X and the prediction y. This is called \"supervised learning\" as we will feed in both X and y to train the model.\n",
    "\n",
    "Let's use a model called Linear Regression. This performs better if we *standardize* the data (0 mean, 1 std).\n",
    "\n",
    "For 2 dimensions this takes the form of \n",
    "\n",
    "\\begin{align}\n",
    "y = m*x + b\n",
    "\\end{align}\n",
    "\n",
    "M is the slope (or coefficient) and b is the intercept.\n",
    "\n",
    "Let's see if we can predict the open price from the ts component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.plot(kind='scatter', x='ts', y='open', figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our input (X) and our labelled data (y) to train our model\n",
    "X = bit_df[['ts']].iloc[:-1]  # drop last row because we don't know what future is\n",
    "y = bit_df.close.shift(-1).iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model and predict output if it were given X\n",
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "pred = lr_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real data, our prediction (blue), and the model from the coeffictient (green shifted)\n",
    "ax = bit_df.plot(kind='scatter', x='ts', y='open', color='black', figsize=(14,10))\n",
    "ax.plot(X, pred, color='blue')  # matplotlib plot\n",
    "ax.plot(X, X*lr_model.coef_ + lr_model.intercept_+ 100, linestyle='--', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical distance between line and point is the error. *Ordinary Least Squares* \n",
    "# regression tries to minimize the square of the distance.\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score is a measure from 0-100\n",
    "# 0 - the model explains none of the variation\n",
    "# 100 - 100% of the variation is explained by the model\n",
    "print(r2_score(y, pred))\n",
    "# Not that the .score method gives the same value\n",
    "print(lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Regression\n",
    "\n",
    "* Use linear regression to predict ``area`` from the other columns. (If you have ``object`` data columns, you can create *dummy columns* using ``pd.get_dummies``, ``pd.concat``, and ``pd.drop``)\n",
    "* What is your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus - Visualize the errors\n",
    "You can plot the actuals and the predicted values. It looks like our model does a pretty poor job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error plot from Yellowbrick\n",
    "# plot of actual (blue) vs predicted (black dash)\n",
    "# ideally would be around 45 degree line\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "err_viz = PredictionError(lr_model)\n",
    "# Model is already fit\n",
    "#err_viz.fit(X, y)\n",
    "err_viz.score(X, y)\n",
    "err_viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "y_df = pd.DataFrame(y)\n",
    "y_df['pred'] = pred\n",
    "y_df['err'] = y_df.pred - y_df.close\n",
    "(y_df\n",
    " #.iloc[-50:]\n",
    " .plot(figsize=(14,10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Visualize the errors\n",
    "Plot y and predicted y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Features\n",
    "In an attempt to get a better model we are going to use more features to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last row because we don't know what future is\n",
    "\n",
    "X = (bit_df\n",
    "         .drop(['close'], axis=1)\n",
    "         .iloc[:-1])\n",
    "y = bit_df.close.shift(-1).iloc[:-1]\n",
    "cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The describe method on a dataframe gives a statistical summary of the columns\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to scale the data so that volume and ts don't get more\n",
    "# weight that other values\n",
    "ss = preprocessing.StandardScaler()\n",
    "ss.fit(X)\n",
    "X = ss.transform(X)\n",
    "X = pd.DataFrame(X, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see that the data has a mean close to 0\n",
    "# and a std of 1\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2 = linear_model.LinearRegression()\n",
    "lr_model2.fit(X, y)\n",
    "pred = lr_model2.predict(X)\n",
    "lr_model2.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "y_df = pd.DataFrame(y)\n",
    "y_df['pred'] = pred\n",
    "y_df['err'] = y_df.pred - y_df.close\n",
    "y_df.plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "y_df = pd.DataFrame(y)\n",
    "y_df['pred'] = pred\n",
    "y_df['err'] = y_df.pred - y_df.close\n",
    "y_df.iloc[-50:].plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our scores get worse with recent data\n",
    "lr_model2.score(X[-50:], y[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X.columns, lr_model2.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These coefficients correspond to the columns in X\n",
    "pd.DataFrame(list(zip(X.columns, lr_model2.coef_)), columns=['Feature', 'Coeff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.plot(kind='scatter', x='wp', y='close', figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_df.plot(kind='scatter', x='vol_cur', y='close', figsize=(14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Regression\n",
    "* Try scaling the input and using the log of the area and see if you get a better score.\n",
    "\n",
    "* Examine the coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Split\n",
    "In fact we were cheating, predicting things that we already saw serves little purpose. The model could just memorize the data and get a perfect score. But it wouldn't *generalize* to unseen data.\n",
    "\n",
    "To see how it will perform in the real world we will train on a portion of the data and test on a portion that it hasn't seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.\\\n",
    "    train_test_split(X, y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model2 = linear_model.LinearRegression()\n",
    "lr_model2.fit(X_train, y_train)\n",
    "lr_model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_df2 = pd.DataFrame(y_test)\n",
    "y_df2['pred'] = lr_model2.predict(X_test)\n",
    "y_df2['err'] = y_df2.pred - y_df2.close\n",
    "(\n",
    "y_df2\n",
    " #   .iloc[-50:]\n",
    "    .plot(figsize=(14,10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow brick version\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "err_viz2 = PredictionError(lr_model2)\n",
    "err_viz2.score(X_test, y_test)\n",
    "err_viz2.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Regression with Train/Test Split\n",
    "\n",
    "Split the data into test and training data. What is the score on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Errors with Residual Plots\n",
    "A residual is the difference between the prediction and the actual.\n",
    "If we plot predicted value against residuals, we should get a random\n",
    "distribution. If not, a different model more be better given the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(model, X_train, y_train, X_test, y_test):\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    ax = plt.subplot(111)\n",
    "    plt.scatter(model.predict(X_train), \n",
    "                model.predict(X_train) - y_train, \n",
    "                c='b', alpha=.3,\n",
    "                label='train')\n",
    "    plt.scatter(model.predict(X_test), \n",
    "                model.predict(X_test) - y_test, \n",
    "                color='green', alpha=.3,\n",
    "                label='test')\n",
    "    plt.title('Residual Plot - Train (blue), Test (green)')\n",
    "    plt.ylabel('Residual')\n",
    "    ax.legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot(lr_model2, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yellowbrick version\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "res_viz = ResidualsPlot(lr_model2)\n",
    "res_viz.fit(X_train, y_train)\n",
    "res_viz.score(X_test, y_test)\n",
    "res_viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Residual Plot\n",
    "Make a residual plot of your test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM, Random Forest, & Huber  - Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last row because we don't know what future is\n",
    "\n",
    "X = (bit_df\n",
    "         .drop(['close'], axis=1)\n",
    "         .iloc[:-1])\n",
    "y = bit_df.close.shift(-1).iloc[:-1]\n",
    "cols = X.columns\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "ss.fit(X)\n",
    "X = ss.transform(X)\n",
    "X = pd.DataFrame(X, columns=cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.\\\n",
    "    train_test_split(X, y, test_size=.3, random_state=42)\n",
    "    \n",
    "svm_model = svm.SVR(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_model.score(X_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reg_model(model, df):\n",
    "    # drop last row because we don't know what future is\n",
    "\n",
    "    X = (df\n",
    "             .drop(['close'], axis=1)\n",
    "             .iloc[:-1])\n",
    "    y = df.close.shift(-1).iloc[:-1]\n",
    "    cols = X.columns\n",
    "\n",
    "    ss = preprocessing.StandardScaler()\n",
    "    ss.fit(X)\n",
    "    X = ss.transform(X)\n",
    "    X = pd.DataFrame(X, columns=cols)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.\\\n",
    "        train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "    #svm_model = svm.SVR(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test), X_test, y_test, X_train, y_train    \n",
    "    \n",
    "rf_reg = ensemble.RandomForestRegressor() \n",
    "score, X_test, y_test, X_train, y_train = train_reg_model(rf_reg, bit_df)\n",
    "print(score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_plot(X_test, y_test, model):\n",
    "    y_df3 = pd.DataFrame(y_test)\n",
    "    y_df3['pred'] = model.predict(X_test)\n",
    "    y_df3['err'] = y_df3.pred - y_df3.close\n",
    "    (\n",
    "    y_df3\n",
    "     #   .iloc[-50:]\n",
    "        .plot(figsize=(14,10))\n",
    "    )\n",
    "error_plot(X_test, y_test, rf_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow brick version\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "err_viz3 = PredictionError(rf_reg)\n",
    "err_viz3.score(X_test, y_test)\n",
    "err_viz3.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "residual_plot(rf_reg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_reg = linear_model.HuberRegressor()\n",
    "huber_reg.fit(X_train, y_train)\n",
    "huber_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot(X_test, y_test, huber_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow brick version\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "err_viz4 = PredictionError(huber_reg)\n",
    "err_viz4.score(X_test, y_test)\n",
    "err_viz4.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot(huber_reg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Try using another model (RandomForestRegressor or SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
